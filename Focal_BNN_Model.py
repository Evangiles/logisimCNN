import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import os

# Import base classes from the main BNN model
from BNN_Model import (
    BinarizeWeightSTE, BinarizeActivationSTE, BinarizeActivation, sReLU,
    BinaryLinear, BinaryConv2d, ConstantScaling, CustomGAP,
    load_dataset_from_npy, create_sample_dataset
)

# --- Interactive Data Generator import ---
try:
    from BNN_Model import load_dataset_from_templates
    print("âœ… Interactive Data Generator ê´€ë ¨ í•¨ìˆ˜ ë¡œë“œ ì„±ê³µ")
except ImportError:
    print("âš ï¸ Interactive Data Generator ê´€ë ¨ í•¨ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    load_dataset_from_templates = None

# --- Focal Loss Implementation ---
class FocalLoss(nn.Module):
    """
    Focal Loss êµ¬í˜„
    ë…¼ë¬¸: "Focal Loss for Dense Object Detection" (Lin et al., 2017)
    í´ë˜ìŠ¤ ë¶ˆê· í˜• ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜
    
    FL(p_t) = -Î±_t * (1 - p_t)^Î³ * log(p_t)
    
    Args:
        alpha: í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ (list, tensor, or None)
        gamma: focusing parameter (default: 2.0)
        reduction: 'mean', 'sum', or 'none'
    """
    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.reduction = reduction
        
        if isinstance(alpha, list):
            self.alpha = torch.FloatTensor(alpha)
    
    def forward(self, inputs, targets):
        """
        Args:
            inputs: (N, C) logit ê°’
            targets: (N,) ì •ë‹µ ë ˆì´ë¸”
        """
        # Cross Entropy Loss ê³„ì‚°
        ce_loss = F.cross_entropy(inputs, targets, reduction='none')
        
        # ì†Œí”„íŠ¸ë§¥ìŠ¤ í™•ë¥  ê³„ì‚° (p_t)
        p = torch.exp(-ce_loss)
        
        # Alpha ê°€ì¤‘ì¹˜ ì ìš©
        if self.alpha is not None:
            if self.alpha.device != targets.device:
                self.alpha = self.alpha.to(targets.device)
            alpha_t = self.alpha[targets]
            focal_loss = alpha_t * (1 - p) ** self.gamma * ce_loss
        else:
            focal_loss = (1 - p) ** self.gamma * ce_loss
        
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

# --- Quantization-Aware Trainingì„ ìœ„í•œ STE ---
class TruncationSTE(torch.autograd.Function):
    """
    Straight-Through Estimator for Truncation.
    - Forward pass: truncates the input tensor.
    - Backward pass: passes the gradient as is (identity function).
    """
    @staticmethod
    def forward(ctx, x):
        return torch.trunc(x)

    @staticmethod
    def backward(ctx, grad_output):
        return grad_output

# --- Focal Loss ê¸°ë°˜ BNN ëª¨ë¸ ---
class FocalBNN(nn.Module):
    """
    Focal Lossë¥¼ ì‚¬ìš©í•˜ëŠ” Binary Neural Network
    - í´ë˜ìŠ¤ ë¶ˆê· í˜• ë°ì´í„°ì…‹ì—ì„œ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ ì „ìš© ëª¨ë¸
    - ê¸°ë³¸ êµ¬ì¡°ëŠ” PaperInspiredBNNê³¼ ë™ì¼í•˜ì§€ë§Œ Focal Loss ìµœì í™”ë¨
    """
    def __init__(self, c1_channels=128, num_classes=10, scale_value=None):
        super(FocalBNN, self).__init__()
        
        if scale_value is None:
            scale_value = float(num_classes)
            
        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
        self.conv1 = BinaryConv2d(1, c1_channels, kernel_size=3, padding=0)
        self.srelu1 = sReLU()

        # ìƒìˆ˜ ìŠ¤ì¼€ì¼ë§ ë ˆì´ì–´
        self.constant_scaling = ConstantScaling(scale_value)

        # Custom Global Average Pooling (ë…¼ë¬¸ê³¼ ë™ì¼í•˜ê²Œ 32ë¡œ ë‚˜ëˆ„ê¸°)
        self.global_avg_pool = CustomGAP(divisor=32.0)

        # GAP ì´í›„ sReLU í™œì„±í™” í•¨ìˆ˜
        self.srelu_after_gap = sReLU()

        # ìµœì¢… ë¶„ë¥˜ ë ˆì´ì–´
        self.fc = BinaryLinear(c1_channels, num_classes)
        
        print(f"ğŸ¯ FocalBNN ì´ˆê¸°í™” ì™„ë£Œ:")
        print(f" - ì†ì‹¤ í•¨ìˆ˜: Focal Loss (í´ë˜ìŠ¤ ë¶ˆê· í˜• ëŒ€ì‘)")
        print(f" - ìƒìˆ˜ ìŠ¤ì¼€ì¼ë§ ê°’: {scale_value}")
        print(f" - í™œì„±í™” í•¨ìˆ˜: sReLU (Conv ì´í›„ + GAP ì´í›„)")
        print(f" - ë°°ì¹˜ ì •ê·œí™”: ì‚¬ìš© ì•ˆí•¨")
        print(f" - MaxPooling: ì‚¬ìš© ì•ˆí•¨ (ê³µê°„ ì •ë³´ ë³´ì¡´)")
        print(f" - Global Pooling: CustomGAP (divisor=32.0)")
        print(f" - ì»¨ë³¼ë£¨ì…˜ ë ˆì´ì–´: 1ê°œ (conv1)")
        print(f" - íŒ¨ë”© ê°’: -1.0 (ë°ì´í„° ì¼ê´€ì„±ì„ ìœ„í•´)")
        print(f" - ì´ˆê¸°í™”: He/MSRA ì´ˆê¸°í™”")

    def forward(self, x):
        # ì…ë ¥ reshape: (batch_size, 64) -> (batch_size, 1, 8, 8)
        x = x.view(-1, 1, 8, 8)
        
        # ì²« ë²ˆì§¸ ì»¨ë³¼ë£¨ì…˜ ë¸”ë¡
        x = self.conv1(x)
        x = self.srelu1(x)
        
        # ìƒìˆ˜ ìŠ¤ì¼€ì¼ë§
        x = self.constant_scaling(x)
        
        # Custom Global Average Pooling
        x = self.global_avg_pool(x)
        
        # Quantization-Aware Training: GAP ì´í›„ ì†Œìˆ˜ì  ì œê±° (STE ì ìš©)
        x = TruncationSTE.apply(x)
        
        # GAP ì´í›„ STE ê¸°ë°˜ ì´ì§„í™” í™œì„±í™”
        x = self.srelu_after_gap(x)
        
        x = x.view(x.size(0), -1)  # Flatten
        
        # ìµœì¢… ë¶„ë¥˜
        x = self.fc(x)
        
        return x

# --- í•˜ì´ë¸Œë¦¬ë“œ ì†ì‹¤ í•¨ìˆ˜ ---
class HybridLoss(nn.Module):
    def __init__(self, focal_weight=0.2, focal_alpha=None, focal_gamma=2.0):
        super(HybridLoss, self).__init__()
        self.focal_weight = focal_weight
        self.focal_loss = FocalLoss(alpha=focal_alpha, gamma=focal_gamma, reduction='mean')
        self.ce_loss = nn.CrossEntropyLoss(reduction='mean')
    
    def forward(self, inputs, targets):
        focal_loss_val = self.focal_loss(inputs, targets)
        ce_loss_val = self.ce_loss(inputs, targets)
        total_loss = self.focal_weight * focal_loss_val + (1 - self.focal_weight) * ce_loss_val
        return total_loss

# --- í•˜ì´ë¸Œë¦¬ë“œ í›ˆë ¨ í•¨ìˆ˜ ---
def train_hybrid_bnn_model(model, X_train, y_train, X_test, y_test, 
                          epochs=500, initial_lr=0.001, weight_decay=1e-4,
                          focal_weight=0.2, focal_gamma=2.0):
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)

    X_train_tensor = torch.FloatTensor(X_train).to(device)
    y_train_tensor = torch.LongTensor(y_train).to(device)
    X_test_tensor = torch.FloatTensor(X_test).to(device)
    y_test_tensor = torch.LongTensor(y_test).to(device)

    criterion = HybridLoss(focal_weight=focal_weight, focal_gamma=focal_gamma)
    optimizer = optim.Adam(model.parameters(), lr=initial_lr, weight_decay=weight_decay)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs, eta_min=initial_lr/100)

    train_losses, train_accuracies, test_accuracies = [], [], []

    for epoch in range(epochs):
        model.train()
        optimizer.zero_grad()

        outputs = model(X_train_tensor)
        loss = criterion(outputs, y_train_tensor)
        loss.backward()
        optimizer.step()

        model.eval()
        with torch.no_grad():
            train_outputs = model(X_train_tensor)
            train_pred = torch.argmax(train_outputs, dim=1)
            train_acc = (train_pred == y_train_tensor).float().mean()

            test_outputs = model(X_test_tensor)
            test_pred = torch.argmax(test_outputs, dim=1)
            test_acc = (test_pred == y_test_tensor).float().mean()

        scheduler.step()

        train_losses.append(loss.item())
        train_accuracies.append(train_acc.item())
        test_accuracies.append(test_acc.item())

        if (epoch + 1) % 50 == 0 or epoch < 10:
            print(f'Epoch [{epoch+1:3d}/{epochs}] | Loss: {loss.item():.4f} | '
                  f'Train Acc: {train_acc:.4f} | Test Acc: {test_acc:.4f}')

    return train_losses, train_accuracies, test_accuracies, model

def plot_hybrid_results(train_losses, train_accuracies, test_accuracies):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
    
    ax1.plot(train_losses, 'b-', linewidth=2)
    ax1.set_title('Training Loss (Hybrid)')
    ax1.set_xlabel('Epoch')
    ax1.set_ylabel('Loss')
    ax1.grid(True, alpha=0.3)
    ax1.set_yscale('log')
    
    ax2.plot(train_accuracies, 'b-', label='Train Accuracy', linewidth=2)
    ax2.plot(test_accuracies, 'r-', label='Test Accuracy', linewidth=2)
    ax2.set_title('Accuracy (Hybrid Loss)')
    ax2.set_xlabel('Epoch')
    ax2.set_ylabel('Accuracy')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    ax2.set_ylim([0, 1])
    
    plt.tight_layout()
    plt.show()

# --- ëª¨ë¸ í‰ê°€ í•¨ìˆ˜ ---
def evaluate_focal_model(model, X_test, y_test):
    """Focal Loss BNN ëª¨ë¸ ìƒì„¸ í‰ê°€"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.eval()
    X_test_tensor = torch.FloatTensor(X_test).to(device)

    with torch.no_grad():
        outputs = model(X_test_tensor)
        predictions = torch.argmax(outputs, dim=1)

    print("ğŸ“Š Focal Loss BNN ëª¨ë¸ ìƒì„¸ ë¶„ë¥˜ ì„±ëŠ¥:")
    print(classification_report(y_test, predictions.cpu().numpy(),
                                target_names=[str(i) for i in range(10)]))
    
    # í´ë˜ìŠ¤ë³„ ì •í™•ë„ ë¶„ì„
    cm = confusion_matrix(y_test, predictions.cpu().numpy())
    class_accuracies = cm.diagonal() / cm.sum(axis=1)
    
    print("\nğŸ“ˆ í´ë˜ìŠ¤ë³„ ì •í™•ë„:")
    for i, acc in enumerate(class_accuracies):
        print(f"  í´ë˜ìŠ¤ {i}: {acc:.4f}")
    
    return predictions.cpu().numpy()



if __name__ == "__main__":
    print("ğŸ¯ í•˜ì´ë¸Œë¦¬ë“œ ì†ì‹¤ ê¸°ë°˜ BNN ì‹¤í—˜")
    print("ğŸ“„ ëª©ì : Focal Loss + Cross-Entropy Loss ì¡°í•©ìœ¼ë¡œ ì•ˆì •ì  í•™ìŠµ")
    print("ğŸ¨ Using: Interactive Template Creator Dataset")
    print("="*80)

    # 1. ë°ì´í„°ì…‹ ë¡œë“œ/ìƒì„±
    print("1ï¸âƒ£ ë°ì´í„°ì…‹ ë¡œë“œ/ìƒì„± ì¤‘...")
    
    # ë°©ë²• 1: ì €ì¥ëœ .npy íŒŒì¼ì—ì„œ ë¡œë“œ ì‹œë„
    X_train, X_test, y_train, y_test = load_dataset_from_npy()
    
    # ë°©ë²• 2: .npy íŒŒì¼ì´ ì—†ìœ¼ë©´ templates.jsonì—ì„œ ìƒì„± ì‹œë„  
    if X_train is None and load_dataset_from_templates is not None:
        print("\nğŸ”„ .npy íŒŒì¼ì´ ì—†ì–´ì„œ templates.jsonì—ì„œ ë°ì´í„°ì…‹ ìƒì„±ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        X_train, X_test, y_train, y_test = load_dataset_from_templates(
            samples_per_digit=2500, test_size=0.2, random_state=42
        )
    
    # ë°©ë²• 3: ë‘˜ ë‹¤ ì‹¤íŒ¨í•˜ë©´ ìƒ˜í”Œ ë°ì´í„°ì…‹ ìƒì„±
    if X_train is None:
        print("\nğŸ² í…œí”Œë¦¿ë„ ì—†ì–´ì„œ ìƒ˜í”Œ ë°ì´í„°ì…‹ì„ ìƒì„±í•©ë‹ˆë‹¤...")
        X_train, X_test, y_train, y_test = create_sample_dataset(samples_per_digit=500)
    
    if X_train is None:
        print("âŒ ë°ì´í„°ì…‹ ë¡œë“œ/ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        exit(1)

    # 2. ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸
    print("\n2ï¸âƒ£ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ ë¶„í¬ í™•ì¸...")
    unique_train, counts_train = np.unique(y_train, return_counts=True)
    total_samples = len(y_train)
    
    print(f"ğŸ“Š í›ˆë ¨ ë°ì´í„° í´ë˜ìŠ¤ë³„ ë¶„í¬:")
    for cls, count in zip(unique_train, counts_train):
        percentage = count / total_samples * 100
        print(f"  í´ë˜ìŠ¤ {cls}: {count}ê°œ ({percentage:.1f}%)")

    # 3. í•˜ì´ë¸Œë¦¬ë“œ BNN ëª¨ë¸ í›ˆë ¨
    print("\n3ï¸âƒ£ í•˜ì´ë¸Œë¦¬ë“œ BNN ëª¨ë¸ í›ˆë ¨...")
    c1_ch = 128
    scale_value = 10.0
    
    hybrid_model = FocalBNN(c1_channels=c1_ch, num_classes=10, scale_value=scale_value)
    
    total_params = sum(p.numel() for p in hybrid_model.parameters())
    print(f" ì´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")

    # í›ˆë ¨ ì‹¤í–‰
    train_losses, train_acc, test_acc, trained_model = train_hybrid_bnn_model(
        hybrid_model, X_train, y_train, X_test, y_test,
        epochs=800, initial_lr=0.015, weight_decay=1e-4, focal_weight=0.2
    )

    # 4. ê²°ê³¼ ì‹œê°í™”
    print("\n4ï¸âƒ£ ê²°ê³¼ ì‹œê°í™”...")
    plot_hybrid_results(train_losses, train_acc, test_acc)

    # 5. ëª¨ë¸ í‰ê°€
    print("\n5ï¸âƒ£ ëª¨ë¸ í‰ê°€...")
    predictions = evaluate_focal_model(trained_model, X_test, y_test)

    # 6. ëª¨ë¸ ì €ì¥
    model_save_path = "hybrid_bnn_model.pth"
    torch.save({
        'model_state_dict': trained_model.state_dict(),
        'model_config': {
            'c1_channels': c1_ch,
            'num_classes': 10,
            'scale_value': scale_value
        },
        'training_history': {
            'train_losses': train_losses,
            'train_accuracies': train_acc,
            'test_accuracies': test_acc
        },
        'final_test_accuracy': test_acc[-1],
        'best_test_accuracy': max(test_acc)
    }, model_save_path)
    print(f"\nâœ… ëª¨ë¸ ì €ì¥: {model_save_path}")

    # 7. ìš”ì•½
    print(f"ğŸ“ˆ ìµœì¢… í…ŒìŠ¤íŠ¸ ì •í™•ë„: {test_acc[-1]:.4f}")
    print(f"ğŸ† ìµœê³  í…ŒìŠ¤íŠ¸ ì •í™•ë„: {max(test_acc):.4f}")

    print(f"\nğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ BNN ì‹¤í—˜ ì™„ë£Œ!")

    print(f"\nCUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"í˜„ì¬ GPU: {torch.cuda.get_device_name()}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB") 